{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Tutorial: From Theory to Practice\n",
    "\n",
    "**A comprehensive guide to neural network quantization techniques**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Quantization](#introduction)\n",
    "2. [Quantization Fundamentals](#fundamentals)\n",
    "3. [Ternary Quantization](#ternary)\n",
    "4. [INT8 Quantization](#int8)\n",
    "5. [Mixed Precision](#mixed-precision)\n",
    "6. [QAT vs PTQ](#qat-vs-ptq)\n",
    "7. [Performance Benchmarks](#benchmarks)\n",
    "8. [Practical Guidelines](#guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Quantization {#introduction}\n",
    "\n",
    "**Quantization** is the process of constraining neural network weights and activations from continuous values to a discrete set of values.\n",
    "\n",
    "### Why Quantize?\n",
    "\n",
    "| Benefit | Description | Impact |\n",
    "|---------|-------------|--------|\n",
    "| **Memory** | Reduced bit-width per parameter | 2-16x smaller models |\n",
    "| **Speed** | Faster arithmetic operations | 2-4x faster inference |\n",
    "| **Power** | Lower energy consumption | Critical for edge devices |\n",
    "| **Bandwidth** | Reduced data transfer | Faster model loading |\n",
    "\n",
    "### Quantization Levels\n",
    "\n",
    "```\n",
    "Float32 (baseline)  â†’  32 bits per weight\n",
    "Float16             â†’  16 bits (2x compression)\n",
    "INT8                â†’   8 bits (4x compression)\n",
    "INT4                â†’   4 bits (8x compression)\n",
    "Ternary {-1,0,1}    â†’   2 bits (16x compression)\n",
    "Binary {-1,1}       â†’   1 bit (32x compression)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization Fundamentals {#fundamentals}\n",
    "\n",
    "### 2.1 Uniform Quantization\n",
    "\n",
    "The basic quantization formula:\n",
    "\n",
    "$$\n",
    "Q(x) = \\text{round}\\left(\\frac{x - z}{s}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the floating-point value\n",
    "- $s$ is the scale factor\n",
    "- $z$ is the zero-point\n",
    "- $Q(x)$ is the quantized integer value\n",
    "\n",
    "Dequantization:\n",
    "\n",
    "$$\n",
    "\\tilde{x} = s \\cdot Q(x) + z\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_quantize(x, bits=8, symmetric=True):\n",
    "    \"\"\"\n",
    "    Uniform quantization to n-bit integers\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        bits: Number of bits (e.g., 8 for INT8)\n",
    "        symmetric: Use symmetric quantization (zero-point = 0)\n",
    "    \"\"\"\n",
    "    if symmetric:\n",
    "        # Symmetric: range is [-max_val, max_val]\n",
    "        max_val = torch.abs(x).max()\n",
    "        qmax = 2 ** (bits - 1) - 1\n",
    "        scale = max_val / qmax\n",
    "        zero_point = 0\n",
    "    else:\n",
    "        # Asymmetric: range is [min_val, max_val]\n",
    "        min_val, max_val = x.min(), x.max()\n",
    "        qmax = 2 ** bits - 1\n",
    "        scale = (max_val - min_val) / qmax\n",
    "        zero_point = -torch.round(min_val / scale)\n",
    "    \n",
    "    # Quantize\n",
    "    x_q = torch.round(x / scale + zero_point)\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    if symmetric:\n",
    "        x_q = torch.clamp(x_q, -qmax, qmax)\n",
    "    else:\n",
    "        x_q = torch.clamp(x_q, 0, qmax)\n",
    "    \n",
    "    # Dequantize\n",
    "    x_dq = scale * (x_q - zero_point)\n",
    "    \n",
    "    return x_dq, x_q, scale, zero_point\n",
    "\n",
    "\n",
    "# Demonstrate quantization\n",
    "x = torch.randn(1000) * 3\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].hist(x.numpy(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('Original (Float32)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Quantization at different bit-widths\n",
    "bit_configs = [(8, 'INT8'), (4, 'INT4'), (2, 'Ternary-like')]\n",
    "for idx, (bits, name) in enumerate(bit_configs):\n",
    "    x_dq, x_q, scale, zp = uniform_quantize(x, bits=bits)\n",
    "    \n",
    "    # Dequantized values\n",
    "    ax = axes[0, idx + 1] if idx < 2 else axes[1, 0]\n",
    "    ax.hist(x_dq.numpy(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax.set_title(f'{name} (dequantized)', fontsize=12, fontweight='bold')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    # Quantization error\n",
    "    error = (x - x_dq).abs()\n",
    "    ax = axes[1, idx + 1] if idx < 2 else axes[1, 1]\n",
    "    ax.hist(error.numpy(), bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    ax.set_title(f'{name} Error', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Frequency')\n",
    "\n",
    "# Statistics table\n",
    "axes[1, 2].axis('off')\n",
    "stats_text = \"Quantization Metrics\\n\" + \"=\"*25 + \"\\n\\n\"\n",
    "for bits, name in bit_configs:\n",
    "    x_dq, _, _, _ = uniform_quantize(x, bits=bits)\n",
    "    mse = F.mse_loss(x, x_dq).item()\n",
    "    mae = (x - x_dq).abs().mean().item()\n",
    "    stats_text += f\"{name}:\\n  MSE: {mse:.4f}\\n  MAE: {mae:.4f}\\n\\n\"\n",
    "\n",
    "axes[1, 2].text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "               verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ternary Quantization {#ternary}\n",
    "\n",
    "Ternary quantization constrains weights to **{-1, 0, 1}**, providing extreme compression with minimal accuracy loss.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "1. **Deterministic**: Threshold-based quantization\n",
    "2. **Stochastic**: Probabilistic quantization\n",
    "3. **Learned**: Trainable thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Deterministic Ternary Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryQuantize(torch.autograd.Function):\n",
    "    \"\"\"Ternary quantization with STE\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, threshold=0.5):\n",
    "        output = torch.sign(input)\n",
    "        output[torch.abs(input) < threshold] = 0\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "def stochastic_ternary_quantize(x):\n",
    "    \"\"\"\n",
    "    Stochastic ternary quantization\n",
    "    \n",
    "    Quantizes with probability proportional to value magnitude:\n",
    "    P(Q(x) = sign(x)) = |x|\n",
    "    P(Q(x) = 0) = 1 - |x|\n",
    "    \"\"\"\n",
    "    x_scaled = torch.clamp(torch.abs(x), 0, 1)\n",
    "    prob = torch.rand_like(x)\n",
    "    output = torch.where(prob < x_scaled, torch.sign(x), torch.zeros_like(x))\n",
    "    return output\n",
    "\n",
    "\n",
    "def learned_threshold_quantize(x, threshold):\n",
    "    \"\"\"\n",
    "    Ternary quantization with learned threshold\n",
    "    \n",
    "    Args:\n",
    "        x: Input weights\n",
    "        threshold: Learnable threshold parameter\n",
    "    \"\"\"\n",
    "    t = torch.sigmoid(threshold)  # Ensure threshold in [0, 1]\n",
    "    x_abs = torch.abs(x)\n",
    "    adaptive_t = t * x_abs.mean()  # Adaptive to weight distribution\n",
    "    \n",
    "    output = torch.sign(x)\n",
    "    output[x_abs < adaptive_t] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare Ternary Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample weights\n",
    "weights = torch.randn(10000) * 1.5\n",
    "threshold_param = torch.tensor(0.0)  # For learned threshold\n",
    "\n",
    "# Apply different methods\n",
    "det_quant = TernaryQuantize.apply(weights, 0.5)\n",
    "stoch_quant = stochastic_ternary_quantize(weights)\n",
    "learned_quant = learned_threshold_quantize(weights, threshold_param)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].hist(weights.numpy(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Weights', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "\n",
    "# Quantization methods\n",
    "methods = [\n",
    "    (det_quant, 'Deterministic', 'green'),\n",
    "    (stoch_quant, 'Stochastic', 'orange'),\n",
    "    (learned_quant, 'Learned Threshold', 'purple')\n",
    "]\n",
    "\n",
    "for idx, (quant, name, color) in enumerate(methods):\n",
    "    # Distribution\n",
    "    counts = [(quant == -1).sum().item(), \n",
    "              (quant == 0).sum().item(), \n",
    "              (quant == 1).sum().item()]\n",
    "    \n",
    "    axes[0, idx + 1].bar([-1, 0, 1], counts, color=[color]*3, \n",
    "                         alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    axes[0, idx + 1].set_title(name, fontsize=12, fontweight='bold')\n",
    "    axes[0, idx + 1].set_xticks([-1, 0, 1])\n",
    "    axes[0, idx + 1].set_xlabel('Value', fontsize=11)\n",
    "    \n",
    "    # Scatter plot: original vs quantized\n",
    "    sample_idx = np.random.choice(len(weights), 500, replace=False)\n",
    "    axes[1, idx].scatter(weights[sample_idx].numpy(), quant[sample_idx].numpy(), \n",
    "                        alpha=0.3, s=10, color=color)\n",
    "    axes[1, idx].axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    axes[1, idx].set_title(f'{name} Mapping', fontsize=12, fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Original Weight', fontsize=11)\n",
    "    axes[1, idx].set_ylabel('Quantized Weight', fontsize=11)\n",
    "    axes[1, idx].set_yticks([-1, 0, 1])\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Statistics\n",
    "axes[1, 3].axis('off')\n",
    "stats = \"Sparsity Analysis\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "for quant, name, _ in methods:\n",
    "    sparsity = (quant == 0).sum().item() / len(quant) * 100\n",
    "    stats += f\"{name}:\\n  Sparsity: {sparsity:.1f}%\\n\"\n",
    "    stats += f\"  -1: {(quant == -1).sum().item()}\\n\"\n",
    "    stats += f\"   0: {(quant == 0).sum().item()}\\n\"\n",
    "    stats += f\"  +1: {(quant == 1).sum().item()}\\n\\n\"\n",
    "\n",
    "axes[1, 3].text(0.1, 0.5, stats, fontsize=10, family='monospace',\n",
    "               verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. INT8 Quantization {#int8}\n",
    "\n",
    "INT8 quantization maps floating-point values to 8-bit integers, providing a good balance between compression and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INT8Quantize(torch.autograd.Function):\n",
    "    \"\"\"INT8 quantization with STE\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Calculate scale\n",
    "        max_val = torch.abs(input).max()\n",
    "        scale = max_val / 127.0\n",
    "        \n",
    "        # Quantize to INT8\n",
    "        input_q = torch.round(input / scale)\n",
    "        input_q = torch.clamp(input_q, -127, 127)\n",
    "        \n",
    "        # Dequantize\n",
    "        output = input_q * scale\n",
    "        \n",
    "        ctx.scale = scale\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "class LinearINT8(nn.Module):\n",
    "    \"\"\"Linear layer with INT8 quantization\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight_q = INT8Quantize.apply(self.weight)\n",
    "        return F.linear(x, weight_q, self.bias)\n",
    "\n",
    "\n",
    "# Compare float32 vs INT8\n",
    "weights = torch.randn(5000) * 2\n",
    "weights_int8 = INT8Quantize.apply(weights)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(weights.numpy(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Float32 Weights', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Weight Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# INT8\n",
    "axes[1].hist(weights_int8.numpy(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].set_title('INT8 Quantized', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Weight Value')\n",
    "\n",
    "# Error distribution\n",
    "error = (weights - weights_int8).abs()\n",
    "axes[2].hist(error.numpy(), bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[2].set_title('Quantization Error', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Absolute Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Absolute Error: {error.mean().item():.6f}\")\n",
    "print(f\"Max Absolute Error: {error.max().item():.6f}\")\n",
    "print(f\"SNR: {20 * np.log10(weights.std().item() / error.std().item()):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mixed Precision {#mixed-precision}\n",
    "\n",
    "Mixed precision uses different quantization levels for different layers:\n",
    "\n",
    "- **First layer**: Float32 or Float16 (input sensitivity)\n",
    "- **Middle layers**: Ternary or INT8 (bulk of parameters)\n",
    "- **Last layer**: Float32 (classification accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecisionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixed precision network:\n",
    "    - Layer 1: Float32 (high precision for input)\n",
    "    - Layer 2-3: Ternary (compression)\n",
    "    - Layer 4: Float32 (accuracy for output)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dims=[256, 128, 64], output_dim=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First layer: Full precision\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        \n",
    "        # Middle layers: Ternary\n",
    "        from examples.mnist_ternary import LinearTernary\n",
    "        self.fc2 = LinearTernary(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = LinearTernary(hidden_dims[1], hidden_dims[2])\n",
    "        \n",
    "        # Last layer: Full precision\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def get_precision_info(self):\n",
    "        \"\"\"Get precision information for each layer\"\"\"\n",
    "        info = []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                params = sum(p.numel() for p in module.parameters())\n",
    "                bits = 32\n",
    "                precision = 'Float32'\n",
    "            elif hasattr(module, '__class__') and 'Ternary' in module.__class__.__name__:\n",
    "                params = sum(p.numel() for p in module.parameters())\n",
    "                bits = 2\n",
    "                precision = 'Ternary'\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            size_kb = params * bits / 8 / 1024\n",
    "            info.append({\n",
    "                'Layer': name,\n",
    "                'Precision': precision,\n",
    "                'Bits': bits,\n",
    "                'Parameters': params,\n",
    "                'Size (KB)': size_kb\n",
    "            })\n",
    "        return info\n",
    "\n",
    "\n",
    "# Create model and analyze\n",
    "try:\n",
    "    model_mixed = MixedPrecisionNet()\n",
    "    info = model_mixed.get_precision_info()\n",
    "    \n",
    "    # Create visualization\n",
    "    df = pd.DataFrame(info)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Layer sizes\n",
    "    colors = ['steelblue' if p == 'Float32' else 'green' for p in df['Precision']]\n",
    "    ax1.barh(df['Layer'], df['Size (KB)'], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('Size (KB)', fontsize=12)\n",
    "    ax1.set_title('Layer Size by Precision', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision distribution\n",
    "    precision_sizes = df.groupby('Precision')['Size (KB)'].sum()\n",
    "    ax2.pie(precision_sizes, labels=precision_sizes.index, autopct='%1.1f%%',\n",
    "           colors=['green', 'steelblue'], startangle=90,\n",
    "           textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    ax2.set_title('Storage Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMixed Precision Model Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Size: {df['Size (KB)'].sum():.2f} KB\")\n",
    "    print(f\"Ternary Portion: {precision_sizes.get('Ternary', 0) / precision_sizes.sum() * 100:.1f}%\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Note: Run notebook 01_introduction.ipynb first to have LinearTernary available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. QAT vs PTQ {#qat-vs-ptq}\n",
    "\n",
    "Two main approaches to quantization:\n",
    "\n",
    "### Post-Training Quantization (PTQ)\n",
    "- Quantize after training\n",
    "- Fast and simple\n",
    "- May have accuracy drop\n",
    "\n",
    "### Quantization-Aware Training (QAT)\n",
    "- Simulate quantization during training\n",
    "- Better accuracy preservation\n",
    "- Requires retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTQModel(nn.Module):\n",
    "    \"\"\"Standard model for Post-Training Quantization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def quantize_ptq(self):\n",
    "        \"\"\"Apply post-training quantization\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.data = TernaryQuantize.apply(self.fc1.weight.data)\n",
    "            self.fc2.weight.data = TernaryQuantize.apply(self.fc2.weight.data)\n",
    "\n",
    "\n",
    "class QATModel(nn.Module):\n",
    "    \"\"\"Model with Quantization-Aware Training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Import ternary layer\n",
    "        try:\n",
    "            from examples.mnist_ternary import LinearTernary\n",
    "            self.fc1 = LinearTernary(784, 128)\n",
    "            self.fc2 = LinearTernary(128, 10)\n",
    "        except:\n",
    "            # Fallback\n",
    "            self.fc1 = nn.Linear(784, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Simulate training comparison\n",
    "def simulate_training_curves():\n",
    "    \"\"\"Simulate typical QAT vs PTQ training curves\"\"\"\n",
    "    epochs = np.arange(1, 11)\n",
    "    \n",
    "    # Simulated accuracies (based on typical behavior)\n",
    "    # Float baseline\n",
    "    float_acc = 90 + 8 * (1 - np.exp(-epochs / 3))\n",
    "    \n",
    "    # QAT: gradual learning\n",
    "    qat_acc = 85 + 10 * (1 - np.exp(-epochs / 3.5))\n",
    "    \n",
    "    # PTQ: immediate accuracy drop, then slight recovery\n",
    "    ptq_base = float_acc[-1]  # Start from trained model\n",
    "    ptq_drop = 5  # Immediate drop from quantization\n",
    "    ptq_acc = np.full_like(epochs, ptq_base - ptq_drop, dtype=float)\n",
    "    \n",
    "    return epochs, float_acc, qat_acc, ptq_acc\n",
    "\n",
    "\n",
    "epochs, float_acc, qat_acc, ptq_acc = simulate_training_curves()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Training curves\n",
    "ax1.plot(epochs, float_acc, 'o-', linewidth=2, label='Float32 Baseline', color='blue')\n",
    "ax1.plot(epochs, qat_acc, 's-', linewidth=2, label='QAT (Ternary)', color='green')\n",
    "ax1.axhline(ptq_acc[0], linestyle='--', linewidth=2, label='PTQ (Ternary)', color='red')\n",
    "ax1.fill_between(epochs, ptq_acc[0] - 1, ptq_acc[0] + 1, alpha=0.2, color='red')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('QAT vs PTQ Training Curves', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison table\n",
    "comparison_data = [\n",
    "    ['Method', 'Accuracy', 'Training Time', 'Difficulty'],\n",
    "    ['Float32', '98.0%', 'Baseline', 'Easy'],\n",
    "    ['PTQ', '93.0%', 'None (instant)', 'Very Easy'],\n",
    "    ['QAT', '95.0%', '+20% overhead', 'Moderate']\n",
    "]\n",
    "\n",
    "ax2.axis('tight')\n",
    "ax2.axis('off')\n",
    "table = ax2.table(cellText=comparison_data, cellLoc='left', loc='center',\n",
    "                 colWidths=[0.2, 0.2, 0.3, 0.2])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 3)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax2.set_title('Method Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QAT vs PTQ Summary\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPTQ (Post-Training Quantization):\")\n",
    "print(\"  âœ“ Very fast (no retraining)\")\n",
    "print(\"  âœ“ Simple to implement\")\n",
    "print(\"  âœ— Larger accuracy drop (3-5%)\")\n",
    "print(\"\\nQAT (Quantization-Aware Training):\")\n",
    "print(\"  âœ“ Better accuracy (1-2% drop)\")\n",
    "print(\"  âœ“ Model adapts to quantization\")\n",
    "print(\"  âœ— Requires retraining\")\n",
    "print(\"  âœ— More complex implementation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Benchmarks {#benchmarks}\n",
    "\n",
    "Let's benchmark different quantization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_quantization(input_size=1000, output_size=1000, num_runs=100):\n",
    "    \"\"\"Benchmark different quantization methods\"\"\"\n",
    "    \n",
    "    # Create test data\n",
    "    x = torch.randn(64, input_size).to(device)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Float32 baseline\n",
    "    layer_fp32 = nn.Linear(input_size, output_size).to(device)\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = layer_fp32(x)\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    fp32_time = (time.time() - start) / num_runs * 1000  # ms\n",
    "    fp32_size = sum(p.numel() * 4 for p in layer_fp32.parameters()) / 1024  # KB\n",
    "    \n",
    "    results.append({\n",
    "        'Method': 'Float32',\n",
    "        'Time (ms)': fp32_time,\n",
    "        'Size (KB)': fp32_size,\n",
    "        'Speedup': 1.0,\n",
    "        'Compression': 1.0\n",
    "    })\n",
    "    \n",
    "    # INT8\n",
    "    layer_int8 = LinearINT8(input_size, output_size).to(device)\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = layer_int8(x)\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    int8_time = (time.time() - start) / num_runs * 1000\n",
    "    int8_size = sum(p.numel() * 1 for p in layer_int8.parameters()) / 1024  # 8 bits = 1 byte\n",
    "    \n",
    "    results.append({\n",
    "        'Method': 'INT8',\n",
    "        'Time (ms)': int8_time,\n",
    "        'Size (KB)': int8_size,\n",
    "        'Speedup': fp32_time / int8_time,\n",
    "        'Compression': fp32_size / int8_size\n",
    "    })\n",
    "    \n",
    "    # Ternary (simulated with 2-bit storage)\n",
    "    try:\n",
    "        from examples.mnist_ternary import LinearTernary\n",
    "        layer_ternary = LinearTernary(input_size, output_size).to(device)\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            _ = layer_ternary(x)\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        ternary_time = (time.time() - start) / num_runs * 1000\n",
    "        ternary_size = sum(p.numel() * 0.25 for p in layer_ternary.parameters()) / 1024  # 2 bits\n",
    "        \n",
    "        results.append({\n",
    "            'Method': 'Ternary',\n",
    "            'Time (ms)': ternary_time,\n",
    "            'Size (KB)': ternary_size,\n",
    "            'Speedup': fp32_time / ternary_time,\n",
    "            'Compression': fp32_size / ternary_size\n",
    "        })\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Running benchmarks...\")\n",
    "df_results = benchmark_quantization()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Inference time\n",
    "axes[0].bar(df_results['Method'], df_results['Time (ms)'], \n",
    "           color=['blue', 'green', 'orange'][:len(df_results)],\n",
    "           alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Time (ms)', fontsize=12)\n",
    "axes[0].set_title('Inference Time', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    axes[0].text(i, row['Time (ms)'], f\"{row['Time (ms)']:.3f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Model size\n",
    "axes[1].bar(df_results['Method'], df_results['Size (KB)'],\n",
    "           color=['blue', 'green', 'orange'][:len(df_results)],\n",
    "           alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Size (KB)', fontsize=12)\n",
    "axes[1].set_title('Model Size', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
    "    axes[1].text(i, row['Size (KB)'], f\"{row['Size (KB)']:.1f}\",\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Speedup and compression\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "axes[2].bar(x - width/2, df_results['Speedup'], width, label='Speedup',\n",
    "           color='green', alpha=0.7, edgecolor='black')\n",
    "axes[2].bar(x + width/2, df_results['Compression'], width, label='Compression',\n",
    "           color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_ylabel('Ratio (x)', fontsize=12)\n",
    "axes[2].set_title('Speedup & Compression', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(df_results['Method'])\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTIZATION BENCHMARK RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(df_results.to_string(index=False, float_format=lambda x: f'{x:.3f}'))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Guidelines {#guidelines}\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "#### Ternary Quantization\n",
    "**Best for:**\n",
    "- Extreme memory constraints\n",
    "- Edge devices with limited storage\n",
    "- Models where 2-3% accuracy drop is acceptable\n",
    "- Binary classification tasks\n",
    "\n",
    "**Pros:** 16x compression, very fast inference  \n",
    "**Cons:** 2-4% accuracy drop, requires QAT\n",
    "\n",
    "#### INT8 Quantization\n",
    "**Best for:**\n",
    "- General purpose deployment\n",
    "- Production systems\n",
    "- Models requiring high accuracy\n",
    "- Hardware with INT8 support\n",
    "\n",
    "**Pros:** 4x compression, <1% accuracy drop, PTQ works well  \n",
    "**Cons:** Less compression than ternary\n",
    "\n",
    "#### Mixed Precision\n",
    "**Best for:**\n",
    "- Balancing accuracy and efficiency\n",
    "- Deep networks\n",
    "- When some layers are more sensitive\n",
    "\n",
    "**Pros:** Flexible, good accuracy-efficiency trade-off  \n",
    "**Cons:** More complex to implement and tune\n",
    "\n",
    "### Quantization Workflow:\n",
    "\n",
    "```\n",
    "1. Train baseline Float32 model\n",
    "   â†“\n",
    "2. Try PTQ first (quick baseline)\n",
    "   â†“\n",
    "3. If accuracy drop > 2%:\n",
    "   â†’ Use QAT\n",
    "   â†’ Try mixed precision\n",
    "   â†“\n",
    "4. Fine-tune quantization parameters\n",
    "   â†“\n",
    "5. Benchmark on target hardware\n",
    "```\n",
    "\n",
    "### Tips for Better Quantization:\n",
    "\n",
    "1. **Use Batch Normalization**: Helps stabilize quantized activations\n",
    "2. **Avoid quantizing first/last layers**: They're most sensitive\n",
    "3. **Per-channel quantization**: Better than per-tensor for CNNs\n",
    "4. **Calibration dataset**: Use representative data for PTQ\n",
    "5. **Gradual quantization**: Start with higher bits, gradually reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Fundamentals** of neural network quantization  \n",
    "âœ… **Ternary quantization** methods (deterministic, stochastic, learned)  \n",
    "âœ… **INT8 quantization** for better accuracy-compression trade-offs  \n",
    "âœ… **Mixed precision** strategies for optimal performance  \n",
    "âœ… **QAT vs PTQ** comparison and when to use each  \n",
    "âœ… **Performance benchmarks** across different methods  \n",
    "âœ… **Practical guidelines** for production deployment  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "ðŸ“˜ [03_performance_analysis.ipynb](03_performance_analysis.ipynb) - Deep dive into performance optimization\n",
    "\n",
    "---\n",
    "\n",
    "*Triton DSL - Advanced Quantization for Neural Networks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
