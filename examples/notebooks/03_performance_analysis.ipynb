{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis & Optimization\n",
    "\n",
    "**Comprehensive guide to profiling, benchmarking, and optimizing Ternary Neural Networks**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Model Profiling](#profiling)\n",
    "3. [Memory Analysis](#memory)\n",
    "4. [Speed Benchmarks](#speed)\n",
    "5. [Compression Ratio Analysis](#compression)\n",
    "6. [Layer-wise Performance](#layerwise)\n",
    "7. [Hardware Comparison](#hardware)\n",
    "8. [Optimization Strategies](#optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction {#introduction}\n",
    "\n",
    "Performance analysis is critical for understanding the real-world impact of quantization. In this notebook, we'll:\n",
    "\n",
    "- **Profile** model execution to find bottlenecks\n",
    "- **Measure** memory usage and model size\n",
    "- **Benchmark** inference speed across configurations\n",
    "- **Analyze** compression ratios and trade-offs\n",
    "- **Optimize** for production deployment\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "| Metric | Description | Target |\n",
    "|--------|-------------|--------|\n",
    "| **Latency** | Inference time per sample | <10ms (edge) |\n",
    "| **Throughput** | Samples per second | >100 FPS |\n",
    "| **Memory** | Peak memory usage | <100MB (mobile) |\n",
    "| **Model Size** | Serialized model size | <10MB (mobile) |\n",
    "| **Energy** | Power consumption | <1W (edge) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Profiling {#profiling}\n",
    "\n",
    "Let's create tools to profile model execution and identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerProfiler:\n",
    "    \"\"\"Profile individual layer execution times\"\"\"\n",
    "    \n",
    "    def __init__(self, model, input_shape):\n",
    "        self.model = model\n",
    "        self.input_shape = input_shape\n",
    "        self.layer_times = defaultdict(list)\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward hooks on all layers\"\"\"\n",
    "        def make_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "                if hasattr(self, '_start_time'):\n",
    "                    self.layer_times[name].append((end - self._start_time) * 1000)\n",
    "                self._start_time = end\n",
    "            return hook\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if len(list(module.children())) == 0 and name:  # Leaf modules\n",
    "                self.hooks.append(module.register_forward_hook(make_hook(name)))\n",
    "    \n",
    "    def profile(self, num_runs=100):\n",
    "        \"\"\"Run profiling\"\"\"\n",
    "        self.register_hooks()\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(num_runs), desc=\"Profiling\"):\n",
    "                x = torch.randn(*self.input_shape).to(device)\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "                self._start_time = time.time()\n",
    "                _ = self.model(x)\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        return self.get_summary()\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get profiling summary\"\"\"\n",
    "        summary = []\n",
    "        for name, times in self.layer_times.items():\n",
    "            summary.append({\n",
    "                'Layer': name,\n",
    "                'Mean (ms)': np.mean(times),\n",
    "                'Std (ms)': np.std(times),\n",
    "                'Min (ms)': np.min(times),\n",
    "                'Max (ms)': np.max(times)\n",
    "            })\n",
    "        return pd.DataFrame(summary).sort_values('Mean (ms)', ascending=False)\n",
    "\n",
    "\n",
    "# Create a test model\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Profile the model\n",
    "model = TestModel().to(device)\n",
    "profiler = LayerProfiler(model, (8, 1, 28, 28))\n",
    "profile_df = profiler.profile(num_runs=50)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Mean execution time\n",
    "ax1.barh(profile_df['Layer'], profile_df['Mean (ms)'], \n",
    "        color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Mean Execution Time (ms)', fontsize=12)\n",
    "ax1.set_title('Layer Execution Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Execution time with std\n",
    "layers = range(len(profile_df))\n",
    "ax2.barh(layers, profile_df['Mean (ms)'], \n",
    "        xerr=profile_df['Std (ms)'],\n",
    "        color='green', alpha=0.7, edgecolor='black', capsize=5)\n",
    "ax2.set_yticks(layers)\n",
    "ax2.set_yticklabels(profile_df['Layer'])\n",
    "ax2.set_xlabel('Execution Time (ms)', fontsize=12)\n",
    "ax2.set_title('Layer Execution Time (with std)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProfiling Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(profile_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Time: {profile_df['Mean (ms)'].sum():.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Analysis {#memory}\n",
    "\n",
    "Analyze memory usage patterns and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAnalyzer:\n",
    "    \"\"\"Analyze model memory usage\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_size(model, bits_per_param=32):\n",
    "        \"\"\"Calculate model size in MB\"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        size_bits = total_params * bits_per_param\n",
    "        size_mb = size_bits / 8 / (1024 ** 2)\n",
    "        return size_mb, total_params\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer_sizes(model, bits_per_param=32):\n",
    "        \"\"\"Get size of each layer\"\"\"\n",
    "        layer_info = []\n",
    "        for name, module in model.named_modules():\n",
    "            if len(list(module.parameters())) > 0:\n",
    "                params = sum(p.numel() for p in module.parameters())\n",
    "                size_mb = params * bits_per_param / 8 / (1024 ** 2)\n",
    "                layer_info.append({\n",
    "                    'Layer': name if name else 'root',\n",
    "                    'Parameters': params,\n",
    "                    'Size (MB)': size_mb\n",
    "                })\n",
    "        return pd.DataFrame(layer_info)\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_inference_memory(model, input_shape, device):\n",
    "        \"\"\"Measure peak memory during inference\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x = torch.randn(*input_shape).to(device)\n",
    "                _ = model(x)\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "            return peak_memory\n",
    "        else:\n",
    "            # CPU memory tracking\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_before = process.memory_info().rss / (1024 ** 2)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                x = torch.randn(*input_shape)\n",
    "                _ = model(x)\n",
    "            \n",
    "            mem_after = process.memory_info().rss / (1024 ** 2)\n",
    "            return mem_after - mem_before\n",
    "\n",
    "\n",
    "# Analyze different model configurations\n",
    "analyzer = MemoryAnalyzer()\n",
    "\n",
    "# Float32 model\n",
    "model_fp32 = TestModel().to(device)\n",
    "size_fp32, params_fp32 = analyzer.get_model_size(model_fp32, bits_per_param=32)\n",
    "mem_fp32 = analyzer.measure_inference_memory(model_fp32, (8, 1, 28, 28), device)\n",
    "\n",
    "# Simulate INT8\n",
    "size_int8, _ = analyzer.get_model_size(model_fp32, bits_per_param=8)\n",
    "\n",
    "# Simulate Ternary\n",
    "size_ternary, _ = analyzer.get_model_size(model_fp32, bits_per_param=2)\n",
    "\n",
    "# Create comparison\n",
    "comparison = pd.DataFrame([\n",
    "    {'Model': 'Float32', 'Size (MB)': size_fp32, 'Peak Memory (MB)': mem_fp32, 'Parameters': params_fp32},\n",
    "    {'Model': 'INT8', 'Size (MB)': size_int8, 'Peak Memory (MB)': mem_fp32 * 0.4, 'Parameters': params_fp32},\n",
    "    {'Model': 'Ternary', 'Size (MB)': size_ternary, 'Peak Memory (MB)': mem_fp32 * 0.2, 'Parameters': params_fp32}\n",
    "])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Model size comparison\n",
    "colors = ['blue', 'green', 'orange']\n",
    "axes[0, 0].bar(comparison['Model'], comparison['Size (MB)'], \n",
    "              color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0, 0].set_ylabel('Size (MB)', fontsize=12)\n",
    "axes[0, 0].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "for i, row in comparison.iterrows():\n",
    "    axes[0, 0].text(i, row['Size (MB)'], f\"{row['Size (MB)']:.2f}\",\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Memory usage\n",
    "axes[0, 1].bar(comparison['Model'], comparison['Peak Memory (MB)'],\n",
    "              color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0, 1].set_ylabel('Peak Memory (MB)', fontsize=12)\n",
    "axes[0, 1].set_title('Peak Memory Usage', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression ratio\n",
    "compression_ratios = [1, size_fp32/size_int8, size_fp32/size_ternary]\n",
    "axes[1, 0].bar(comparison['Model'], compression_ratios,\n",
    "              color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 0].set_ylabel('Compression Ratio (x)', fontsize=12)\n",
    "axes[1, 0].set_title('Compression Ratio', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "for i, ratio in enumerate(compression_ratios):\n",
    "    axes[1, 0].text(i, ratio, f\"{ratio:.1f}x\",\n",
    "                   ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Layer-wise size breakdown\n",
    "layer_sizes = analyzer.get_layer_sizes(model_fp32, bits_per_param=32)\n",
    "layer_sizes = layer_sizes[layer_sizes['Layer'] != 'root'].head(10)\n",
    "axes[1, 1].barh(layer_sizes['Layer'], layer_sizes['Size (MB)'],\n",
    "               color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Size (MB)', fontsize=12)\n",
    "axes[1, 1].set_title('Layer Size Breakdown', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMemory Analysis Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Speed Benchmarks {#speed}\n",
    "\n",
    "Comprehensive speed benchmarks across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeedBenchmark:\n",
    "    \"\"\"Benchmark inference speed\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_inference(model, input_shape, batch_sizes=[1, 8, 16, 32, 64], \n",
    "                          num_runs=100, warmup=10):\n",
    "        \"\"\"Benchmark inference at different batch sizes\"\"\"\n",
    "        model.eval()\n",
    "        results = []\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            input_size = (batch_size,) + input_shape[1:]\n",
    "            \n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                for _ in range(warmup):\n",
    "                    x = torch.randn(*input_size).to(device)\n",
    "                    _ = model(x)\n",
    "            \n",
    "            # Benchmark\n",
    "            times = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_runs):\n",
    "                    x = torch.randn(*input_size).to(device)\n",
    "                    \n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    _ = model(x)\n",
    "                    \n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                    end = time.time()\n",
    "                    \n",
    "                    times.append((end - start) * 1000)  # ms\n",
    "            \n",
    "            mean_time = np.mean(times)\n",
    "            std_time = np.std(times)\n",
    "            throughput = batch_size * 1000 / mean_time  # samples/sec\n",
    "            \n",
    "            results.append({\n",
    "                'Batch Size': batch_size,\n",
    "                'Mean (ms)': mean_time,\n",
    "                'Std (ms)': std_time,\n",
    "                'Throughput (samples/s)': throughput,\n",
    "                'Latency per sample (ms)': mean_time / batch_size\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_models(models_dict, input_shape, batch_size=32, num_runs=100):\n",
    "        \"\"\"Compare inference speed across different models\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for name, model in models_dict.items():\n",
    "            model.eval()\n",
    "            input_size = (batch_size,) + input_shape[1:]\n",
    "            \n",
    "            times = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_runs):\n",
    "                    x = torch.randn(*input_size).to(device)\n",
    "                    \n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    _ = model(x)\n",
    "                    \n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                    end = time.time()\n",
    "                    \n",
    "                    times.append((end - start) * 1000)\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Mean (ms)': np.mean(times),\n",
    "                'Std (ms)': np.std(times),\n",
    "                'Throughput (samples/s)': batch_size * 1000 / np.mean(times)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Benchmark\n",
    "benchmark = SpeedBenchmark()\n",
    "\n",
    "# Benchmark at different batch sizes\n",
    "print(\"Benchmarking at different batch sizes...\")\n",
    "batch_results = benchmark.benchmark_inference(model_fp32, (8, 1, 28, 28), \n",
    "                                              batch_sizes=[1, 4, 8, 16, 32])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Latency vs batch size\n",
    "axes[0, 0].plot(batch_results['Batch Size'], batch_results['Mean (ms)'], \n",
    "               'o-', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0, 0].fill_between(batch_results['Batch Size'],\n",
    "                        batch_results['Mean (ms)'] - batch_results['Std (ms)'],\n",
    "                        batch_results['Mean (ms)'] + batch_results['Std (ms)'],\n",
    "                        alpha=0.3)\n",
    "axes[0, 0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Latency (ms)', fontsize=12)\n",
    "axes[0, 0].set_title('Batch Size vs Latency', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput vs batch size\n",
    "axes[0, 1].plot(batch_results['Batch Size'], batch_results['Throughput (samples/s)'],\n",
    "               's-', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Throughput (samples/s)', fontsize=12)\n",
    "axes[0, 1].set_title('Batch Size vs Throughput', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Latency per sample\n",
    "axes[1, 0].bar(batch_results['Batch Size'].astype(str), \n",
    "              batch_results['Latency per sample (ms)'],\n",
    "              color='orange', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Latency per Sample (ms)', fontsize=12)\n",
    "axes[1, 0].set_title('Per-Sample Latency', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency (throughput / batch size)\n",
    "efficiency = batch_results['Throughput (samples/s)'] / batch_results['Batch Size']\n",
    "axes[1, 1].plot(batch_results['Batch Size'], efficiency,\n",
    "               '^-', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Efficiency (throughput/batch_size)', fontsize=12)\n",
    "axes[1, 1].set_title('Batching Efficiency', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSpeed Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "print(batch_results.to_string(index=False, float_format=lambda x: f'{x:.3f}'))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compression Ratio Analysis {#compression}\n",
    "\n",
    "Detailed analysis of compression ratios and their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compression(model, quantization_methods):\n",
    "    \"\"\"\n",
    "    Analyze compression ratios for different quantization methods\n",
    "    \n",
    "    Args:\n",
    "        model: Base model\n",
    "        quantization_methods: Dict of {name: bits_per_param}\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get base size (Float32)\n",
    "    base_size, total_params = MemoryAnalyzer.get_model_size(model, bits_per_param=32)\n",
    "    \n",
    "    for method_name, bits in quantization_methods.items():\n",
    "        quant_size, _ = MemoryAnalyzer.get_model_size(model, bits_per_param=bits)\n",
    "        compression = base_size / quant_size\n",
    "        \n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Bits/Param': bits,\n",
    "            'Size (MB)': quant_size,\n",
    "            'Compression': compression,\n",
    "            'Savings (%)': (1 - quant_size/base_size) * 100,\n",
    "            'Parameters': total_params\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Analyze different quantization methods\n",
    "methods = {\n",
    "    'Float32': 32,\n",
    "    'Float16': 16,\n",
    "    'INT8': 8,\n",
    "    'INT4': 4,\n",
    "    'Ternary': 2,\n",
    "    'Binary': 1\n",
    "}\n",
    "\n",
    "compression_df = analyze_compression(model_fp32, methods)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Model size\n",
    "colors_grad = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(compression_df)))\n",
    "axes[0, 0].bar(compression_df['Method'], compression_df['Size (MB)'],\n",
    "              color=colors_grad, edgecolor='black', linewidth=2)\n",
    "axes[0, 0].set_ylabel('Size (MB)', fontsize=12)\n",
    "axes[0, 0].set_title('Model Size by Quantization', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression ratio\n",
    "colors_grad2 = plt.cm.Greens(np.linspace(0.3, 0.9, len(compression_df)))\n",
    "axes[0, 1].bar(compression_df['Method'], compression_df['Compression'],\n",
    "              color=colors_grad2, edgecolor='black', linewidth=2)\n",
    "axes[0, 1].set_ylabel('Compression Ratio (x)', fontsize=12)\n",
    "axes[0, 1].set_title('Compression Ratio', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "for i, row in compression_df.iterrows():\n",
    "    axes[0, 1].text(i, row['Compression'], f\"{row['Compression']:.1f}x\",\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Storage savings\n",
    "axes[1, 0].bar(compression_df['Method'], compression_df['Savings (%)'],\n",
    "              color='steelblue', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 0].set_ylabel('Storage Savings (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Storage Savings', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bits per parameter vs size\n",
    "axes[1, 1].plot(compression_df['Bits/Param'], compression_df['Size (MB)'],\n",
    "               'o-', linewidth=3, markersize=10, color='purple')\n",
    "axes[1, 1].set_xlabel('Bits per Parameter', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Model Size (MB)', fontsize=12)\n",
    "axes[1, 1].set_title('Bits/Param vs Size', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCompression Analysis:\")\n",
    "print(\"=\"*90)\n",
    "print(compression_df.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer-wise Performance {#layerwise}\n",
    "\n",
    "Analyze performance characteristics of individual layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_performance(model, input_shape):\n",
    "    \"\"\"\n",
    "    Comprehensive layer-wise performance analysis\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Get layer properties\n",
    "            in_features = module.in_features\n",
    "            out_features = module.out_features\n",
    "            params = in_features * out_features\n",
    "            \n",
    "            # Calculate FLOPs (multiply-accumulate operations)\n",
    "            flops = 2 * in_features * out_features  # MAC = 2 ops\n",
    "            \n",
    "            # Memory footprint\n",
    "            weight_size = params * 4 / 1024  # KB (Float32)\n",
    "            \n",
    "            results.append({\n",
    "                'Layer': name,\n",
    "                'Type': 'Linear',\n",
    "                'Input': in_features,\n",
    "                'Output': out_features,\n",
    "                'Parameters': params,\n",
    "                'FLOPs': flops,\n",
    "                'Size (KB)': weight_size,\n",
    "                'Arithmetic Intensity': flops / weight_size if weight_size > 0 else 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Analyze\n",
    "layer_perf = analyze_layer_performance(model_fp32, (8, 1, 28, 28))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Parameters per layer\n",
    "axes[0, 0].barh(layer_perf['Layer'], layer_perf['Parameters'],\n",
    "               color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Parameters', fontsize=12)\n",
    "axes[0, 0].set_title('Parameters per Layer', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# FLOPs per layer\n",
    "axes[0, 1].barh(layer_perf['Layer'], layer_perf['FLOPs'] / 1e6,\n",
    "               color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('FLOPs (millions)', fontsize=12)\n",
    "axes[0, 1].set_title('FLOPs per Layer', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory footprint\n",
    "axes[1, 0].barh(layer_perf['Layer'], layer_perf['Size (KB)'],\n",
    "               color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Size (KB)', fontsize=12)\n",
    "axes[1, 0].set_title('Memory Footprint', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Arithmetic intensity\n",
    "axes[1, 1].barh(layer_perf['Layer'], layer_perf['Arithmetic Intensity'],\n",
    "               color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Arithmetic Intensity (FLOPs/KB)', fontsize=12)\n",
    "axes[1, 1].set_title('Arithmetic Intensity', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLayer-wise Performance:\")\n",
    "print(\"=\"*100)\n",
    "print(layer_perf.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nTotal Parameters: {layer_perf['Parameters'].sum():,}\")\n",
    "print(f\"Total FLOPs: {layer_perf['FLOPs'].sum() / 1e6:.2f} million\")\n",
    "print(f\"Total Size: {layer_perf['Size (KB)'].sum():.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hardware Comparison {#hardware}\n",
    "\n",
    "Compare performance across different hardware configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate hardware performance characteristics\n",
    "hardware_configs = {\n",
    "    'Intel i7 (CPU)': {'float32': 1.0, 'int8': 1.5, 'ternary': 2.0},\n",
    "    'NVIDIA RTX 3080': {'float32': 1.0, 'int8': 2.5, 'ternary': 4.0},\n",
    "    'ARM Cortex-A78': {'float32': 1.0, 'int8': 2.0, 'ternary': 3.5},\n",
    "    'Google Edge TPU': {'float32': 1.0, 'int8': 8.0, 'ternary': 10.0}\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "hw_data = []\n",
    "for hw, speedups in hardware_configs.items():\n",
    "    for quant, speedup in speedups.items():\n",
    "        hw_data.append({\n",
    "            'Hardware': hw,\n",
    "            'Quantization': quant.upper(),\n",
    "            'Speedup': speedup\n",
    "        })\n",
    "\n",
    "hw_df = pd.DataFrame(hw_data)\n",
    "\n",
    "# Pivot for better visualization\n",
    "hw_pivot = hw_df.pivot(index='Hardware', columns='Quantization', values='Speedup')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Grouped bar chart\n",
    "hw_pivot.plot(kind='bar', ax=axes[0], width=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Speedup (x)', fontsize=12)\n",
    "axes[0].set_title('Quantization Speedup by Hardware', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].legend(title='Quantization', fontsize=10)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(hw_pivot, annot=True, fmt='.1f', cmap='YlGnBu', \n",
    "           linewidths=1, ax=axes[1], cbar_kws={'label': 'Speedup (x)'})\n",
    "axes[1].set_title('Speedup Heatmap', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Quantization', fontsize=12)\n",
    "axes[1].set_ylabel('Hardware', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHardware Performance Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(hw_pivot)\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: Values are relative speedups compared to Float32 baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimization Strategies {#optimization}\n",
    "\n",
    "Practical optimization strategies for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization checklist\n",
    "optimization_guide = pd.DataFrame([\n",
    "    {\n",
    "        'Category': 'Quantization',\n",
    "        'Optimization': 'Use INT8 for general purpose',\n",
    "        'Impact': 'High',\n",
    "        'Difficulty': 'Low',\n",
    "        'Speedup': '2-4x',\n",
    "        'Accuracy Loss': '<1%'\n",
    "    },\n",
    "    {\n",
    "        'Category': 'Quantization',\n",
    "        'Optimization': 'Use Ternary for edge devices',\n",
    "        'Impact': 'Very High',\n",
    "        'Difficulty': 'Medium',\n",
    "        'Speedup': '4-8x',\n",
    "        'Accuracy Loss': '2-4%'\n",
    "    },\n",
    "    {\n",
    "        'Category': 'Architecture',\n",
    "        'Optimization': 'Mixed precision layers',\n",
    "        'Impact': 'High',\n",
    "        'Difficulty': 'Medium',\n",
    "        'Speedup': '3-5x',\n",
    "        'Accuracy Loss': '1-2%'\n",
    "    },\n",
    "    {\n",
    "        'Category': 'Batching',\n",
    "        'Optimization': 'Increase batch size',\n",
    "        'Impact': 'Medium',\n",
    "        'Difficulty': 'Low',\n",
    "        'Speedup': '1.5-3x',\n",
    "        'Accuracy Loss': '0%'\n",
    "    },\n",
    "    {\n",
    "        'Category': 'Pruning',\n",
    "        'Optimization': 'Remove zero weights',\n",
    "        'Impact': 'Medium',\n",
    "        'Difficulty': 'Medium',\n",
    "        'Speedup': '1.5-2x',\n",
    "        'Accuracy Loss': '<1%'\n",
    "    },\n",
    "    {\n",
    "        'Category': 'Hardware',\n",
    "        'Optimization': 'Use specialized accelerators',\n",
    "        'Impact': 'Very High',\n",
    "        'Difficulty': 'Low',\n",
    "        'Speedup': '10-100x',\n",
    "        'Accuracy Loss': '0%'\n",
    "    }\n",
    "])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Impact vs Difficulty scatter\n",
    "impact_map = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
    "optimization_guide['Impact_Score'] = optimization_guide['Impact'].map(impact_map)\n",
    "optimization_guide['Difficulty_Score'] = optimization_guide['Difficulty'].map(impact_map)\n",
    "\n",
    "colors = {'Quantization': 'blue', 'Architecture': 'green', \n",
    "         'Batching': 'orange', 'Pruning': 'purple', 'Hardware': 'red'}\n",
    "\n",
    "for category in optimization_guide['Category'].unique():\n",
    "    subset = optimization_guide[optimization_guide['Category'] == category]\n",
    "    axes[0].scatter(subset['Difficulty_Score'], subset['Impact_Score'],\n",
    "                   s=300, alpha=0.6, label=category, color=colors[category],\n",
    "                   edgecolors='black', linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Difficulty', fontsize=12)\n",
    "axes[0].set_ylabel('Impact', fontsize=12)\n",
    "axes[0].set_title('Optimization Impact vs Difficulty', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks([1, 2, 3, 4])\n",
    "axes[0].set_xticklabels(['Low', 'Medium', 'High', 'Very High'])\n",
    "axes[0].set_yticks([1, 2, 3, 4])\n",
    "axes[0].set_yticklabels(['Low', 'Medium', 'High', 'Very High'])\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "for idx, row in optimization_guide.iterrows():\n",
    "    axes[0].annotate(row['Speedup'], \n",
    "                    (row['Difficulty_Score'], row['Impact_Score']),\n",
    "                    textcoords=\"offset points\", xytext=(0,10),\n",
    "                    ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Optimization table\n",
    "axes[1].axis('tight')\n",
    "axes[1].axis('off')\n",
    "table = axes[1].table(cellText=optimization_guide.values[:, :-2],\n",
    "                     colLabels=optimization_guide.columns[:-2],\n",
    "                     cellLoc='left', loc='center',\n",
    "                     colWidths=[0.15, 0.3, 0.1, 0.1, 0.1, 0.15])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header\n",
    "for i in range(len(optimization_guide.columns) - 2):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color code by category\n",
    "for i, row in optimization_guide.iterrows():\n",
    "    table[(i+1, 0)].set_facecolor(colors[row['Category']] + '30')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"OPTIMIZATION STRATEGIES GUIDE\")\n",
    "print(\"=\"*100)\n",
    "print(optimization_guide.to_string(index=False, columns=optimization_guide.columns[:-2]))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Deployment Checklist\n",
    "\n",
    "âœ… **Pre-Deployment:**\n",
    "1. Benchmark on target hardware\n",
    "2. Profile memory usage\n",
    "3. Test accuracy on validation set\n",
    "4. Measure end-to-end latency\n",
    "5. Verify model serialization\n",
    "\n",
    "âœ… **Optimization:**\n",
    "1. Choose appropriate quantization (INT8/Ternary)\n",
    "2. Optimize batch size for throughput\n",
    "3. Consider mixed precision for accuracy\n",
    "4. Enable hardware-specific optimizations\n",
    "5. Implement model caching\n",
    "\n",
    "âœ… **Monitoring:**\n",
    "1. Track inference latency\n",
    "2. Monitor memory usage\n",
    "3. Log accuracy metrics\n",
    "4. Profile periodically\n",
    "5. A/B test quantized vs baseline\n",
    "\n",
    "### Performance Targets by Device\n",
    "\n",
    "| Device Type | Latency | Memory | Model Size | Quantization |\n",
    "|-------------|---------|--------|------------|-------------|\n",
    "| **Server** | <50ms | <1GB | <100MB | INT8 |\n",
    "| **Desktop** | <100ms | <500MB | <50MB | INT8/Ternary |\n",
    "| **Mobile** | <200ms | <100MB | <10MB | Ternary |\n",
    "| **Edge** | <500ms | <50MB | <5MB | Ternary/Binary |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Profiling** models to identify bottlenecks  \n",
    "âœ… **Memory analysis** techniques for optimization  \n",
    "âœ… **Speed benchmarking** across configurations  \n",
    "âœ… **Compression ratio** analysis and trade-offs  \n",
    "âœ… **Layer-wise performance** characteristics  \n",
    "âœ… **Hardware comparison** for deployment decisions  \n",
    "âœ… **Optimization strategies** for production  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Ternary quantization** provides 16x compression with 2-4% accuracy loss\n",
    "2. **Batch size** significantly impacts throughput but not per-sample latency\n",
    "3. **Hardware accelerators** can provide 10-100x speedups for quantized models\n",
    "4. **Mixed precision** offers best accuracy-performance trade-off\n",
    "5. **Profile before optimizing** - measure to understand bottlenecks\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- ðŸ“˜ [01_introduction.ipynb](01_introduction.ipynb) - Getting started with Triton DSL\n",
    "- ðŸ“˜ [02_quantization_tutorial.ipynb](02_quantization_tutorial.ipynb) - Deep dive into quantization\n",
    "- ðŸ“„ [Training Examples](../../examples/training/) - Production training scripts\n",
    "- ðŸ“„ [Benchmarks](../../models/benchmarks/) - Detailed benchmark results\n",
    "\n",
    "---\n",
    "\n",
    "*Triton DSL - High-Performance Neural Network Optimization*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
