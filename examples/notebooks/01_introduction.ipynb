{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Triton DSL\n",
    "\n",
    "**A Domain-Specific Language for Ternary Neural Networks**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Overview](#overview)\n",
    "2. [Why Ternary Neural Networks?](#why-ternary)\n",
    "3. [Installation & Setup](#installation)\n",
    "4. [Quick Start Tutorial](#quick-start)\n",
    "5. [MNIST Example](#mnist-example)\n",
    "6. [Interactive Visualizations](#visualizations)\n",
    "7. [Next Steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview {#overview}\n",
    "\n",
    "Triton is a high-performance Domain-Specific Language (DSL) designed to optimize **Ternary Neural Networks (TNNs)** by enforcing ternary constraints at the syntax level. This enables significant memory density improvements over standard floating-point representations.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- ðŸŽ¯ **Native Ternary Type System**: Built-in `trit` primitive and `TernaryTensor` data structures\n",
    "- âš¡ **Zero-Cost Abstractions**: Compile-time type checking with runtime efficiency  \n",
    "- ðŸš€ **Hardware Optimization**: 2-bit packed storage, CUDA kernels, zero-skipping\n",
    "- ðŸ”— **PyTorch Integration**: Seamless transpilation to PyTorch modules\n",
    "\n",
    "### Architecture Pipeline\n",
    "\n",
    "```\n",
    "Triton Source (.tri)\n",
    "    â†“\n",
    "Lexer/Parser â†’ AST\n",
    "    â†“\n",
    "Type Checker\n",
    "    â†“\n",
    "Code Generator â†’ PyTorch/CUDA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Ternary Neural Networks? {#why-ternary}\n",
    "\n",
    "Ternary Neural Networks constrain weights to **{-1, 0, 1}**, providing:\n",
    "\n",
    "### Memory Savings\n",
    "\n",
    "- **Float32**: 32 bits per weight\n",
    "- **Ternary**: 2 bits per weight  \n",
    "- **Compression**: 16x theoretical, 4-6x practical\n",
    "\n",
    "### Speed Improvements\n",
    "\n",
    "- Zero-skipping: Skip computations for zero weights\n",
    "- Simpler operations: Multiplication becomes addition/subtraction\n",
    "- **2-3x faster inference** on optimized hardware\n",
    "\n",
    "### Accuracy Trade-offs\n",
    "\n",
    "| Model | Float32 | Ternary | Accuracy Drop |\n",
    "|-------|---------|---------|---------------|\n",
    "| MNIST | 98.5% | 96-97% | 1-2% |\n",
    "| CIFAR-10 | 92% | 88-90% | 2-4% |\n",
    "| ImageNet (ResNet-18) | 70% | 65-67% | 3-5% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Installation & Setup {#installation}\n",
    "\n",
    "Let's set up the environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in the correct directory\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add Triton root to path\n",
    "triton_root = Path.cwd().parent.parent\n",
    "if str(triton_root) not in sys.path:\n",
    "    sys.path.insert(0, str(triton_root))\n",
    "\n",
    "print(f\"Triton Root: {triton_root}\")\n",
    "print(f\"Python Version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "# !pip install torch torchvision numpy matplotlib seaborn scikit-learn tqdm\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Start Tutorial {#quick-start}\n",
    "\n",
    "Let's start with a simple example of defining a ternary layer in Triton DSL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Triton DSL Syntax\n",
    "\n",
    "Here's a simple ternary layer definition in Triton DSL:\n",
    "\n",
    "```triton\n",
    "layer SimpleTernary(in_features: int, out_features: int) -> TernaryTensor {\n",
    "    let W: TernaryTensor = random_ternary([out_features, in_features])\n",
    "    \n",
    "    fn forward(x: Tensor[float16]) -> Tensor[float16] {\n",
    "        let output = ternary_matmul(x, W)\n",
    "        return output\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- `TernaryTensor`: Type-safe tensor constrained to {-1, 0, 1}\n",
    "- `random_ternary()`: Initializes weights to random ternary values\n",
    "- `ternary_matmul()`: Optimized matrix multiplication for ternary weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 PyTorch Backend Implementation\n",
    "\n",
    "Let's implement the core ternary quantization primitive in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryQuantize(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Ternary quantization with Straight-Through Estimator (STE).\n",
    "    \n",
    "    Forward: Quantize weights to {-1, 0, 1}\n",
    "    Backward: Pass gradients straight through\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, threshold=0.5):\n",
    "        \"\"\"Quantize to ternary values {-1, 0, 1}\"\"\"\n",
    "        output = torch.sign(input)\n",
    "        output[torch.abs(input) < threshold] = 0\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Straight-through estimator: pass gradients unchanged\"\"\"\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class LinearTernary(nn.Module):\n",
    "    \"\"\"Ternary Linear Layer - weights constrained to {-1, 0, 1}\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights as float for training\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Quantize weights during forward pass\n",
    "        weight_ternary = TernaryQuantize.apply(self.weight)\n",
    "        return F.linear(x, weight_ternary, self.bias)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}'\n",
    "\n",
    "\n",
    "# Test the layer\n",
    "layer = LinearTernary(10, 5)\n",
    "x = torch.randn(2, 10)\n",
    "output = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nWeight values (before quantization): {layer.weight[0, :5].detach().numpy()}\")\n",
    "print(f\"Quantized weights: {TernaryQuantize.apply(layer.weight)[0, :5].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualizing Quantization\n",
    "\n",
    "Let's visualize how weights are quantized to ternary values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random weights\n",
    "weights = torch.randn(1000) * 2\n",
    "quantized = TernaryQuantize.apply(weights)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original distribution\n",
    "axes[0].hist(weights.numpy(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(-0.5, color='red', linestyle='--', label='Threshold')\n",
    "axes[0].axvline(0.5, color='red', linestyle='--')\n",
    "axes[0].set_title('Original Weight Distribution')\n",
    "axes[0].set_xlabel('Weight Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Quantized distribution\n",
    "axes[1].hist(quantized.numpy(), bins=[-1.5, -0.5, 0.5, 1.5], alpha=0.7, \n",
    "             color='green', edgecolor='black')\n",
    "axes[1].set_title('Quantized Weight Distribution')\n",
    "axes[1].set_xlabel('Weight Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xticks([-1, 0, 1])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot showing quantization mapping\n",
    "sample_idx = torch.arange(100)\n",
    "axes[2].scatter(weights[sample_idx].numpy(), quantized[sample_idx].numpy(), \n",
    "               alpha=0.5, s=30)\n",
    "axes[2].axhline(0, color='gray', linestyle='-', linewidth=0.5)\n",
    "axes[2].axvline(-0.5, color='red', linestyle='--', alpha=0.5)\n",
    "axes[2].axvline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('Quantization Mapping')\n",
    "axes[2].set_xlabel('Original Weight')\n",
    "axes[2].set_ylabel('Quantized Weight')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nQuantization Statistics:\")\n",
    "print(f\"  -1 values: {(quantized == -1).sum().item()} ({(quantized == -1).sum().item()/len(quantized)*100:.1f}%)\")\n",
    "print(f\"   0 values: {(quantized == 0).sum().item()} ({(quantized == 0).sum().item()/len(quantized)*100:.1f}%)\")\n",
    "print(f\"  +1 values: {(quantized == 1).sum().item()} ({(quantized == 1).sum().item()/len(quantized)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MNIST Example {#mnist-example}\n",
    "\n",
    "Let's build a complete ternary neural network for MNIST digit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define the Ternary Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryNet(nn.Module):\n",
    "    \"\"\"Simple Ternary Neural Network for MNIST\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = LinearTernary(784, 256)\n",
    "        self.fc2 = LinearTernary(256, 128)\n",
    "        self.fc3 = LinearTernary(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = TernaryNet().to(device)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "# Plot samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(example_data[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {example_targets[i].item()}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Images', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training configuration\n",
    "epochs = 5\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Ternary Neural Network...\\n\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Visualizations {#visualizations}\n",
    "\n",
    "Let's visualize the training progress and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', marker='s', linewidth=2)\n",
    "axes[0].set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', marker='s', linewidth=2)\n",
    "axes[1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Weight Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights from first layer\n",
    "weights_float = model.fc1.weight.detach().cpu()\n",
    "weights_ternary = TernaryQuantize.apply(weights_float)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Float weights distribution\n",
    "axes[0].hist(weights_float.flatten().numpy(), bins=50, alpha=0.7, \n",
    "            color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Float Weights (Layer 1)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Weight Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ternary weights distribution\n",
    "ternary_counts = [(weights_ternary == -1).sum().item(),\n",
    "                  (weights_ternary == 0).sum().item(),\n",
    "                  (weights_ternary == 1).sum().item()]\n",
    "axes[1].bar([-1, 0, 1], ternary_counts, color=['red', 'gray', 'green'], \n",
    "           alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Ternary Weights (Layer 1)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Weight Value')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticks([-1, 0, 1])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sparsity visualization\n",
    "sparsity_per_layer = []\n",
    "layer_names = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LinearTernary):\n",
    "        w = TernaryQuantize.apply(module.weight.detach().cpu())\n",
    "        sparsity = (w == 0).sum().item() / w.numel() * 100\n",
    "        sparsity_per_layer.append(sparsity)\n",
    "        layer_names.append(name)\n",
    "\n",
    "axes[2].bar(range(len(sparsity_per_layer)), sparsity_per_layer, \n",
    "           color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Sparsity by Layer', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Layer')\n",
    "axes[2].set_ylabel('Sparsity (%)')\n",
    "axes[2].set_xticks(range(len(layer_names)))\n",
    "axes[2].set_xticklabels(layer_names, rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWeight Statistics (Layer 1):\")\n",
    "print(f\"  -1 weights: {ternary_counts[0]} ({ternary_counts[0]/weights_ternary.numel()*100:.1f}%)\")\n",
    "print(f\"   0 weights: {ternary_counts[1]} ({ternary_counts[1]/weights_ternary.numel()*100:.1f}%)\")\n",
    "print(f\"  +1 weights: {ternary_counts[2]} ({ternary_counts[2]/weights_ternary.numel()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test batch\n",
    "test_examples = iter(test_loader)\n",
    "test_data, test_targets = next(test_examples)\n",
    "test_data, test_targets = test_data.to(device), test_targets.to(device)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data)\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "\n",
    "# Plot samples with predictions\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(test_data):\n",
    "        img = test_data[i].cpu().squeeze()\n",
    "        true_label = test_targets[i].cpu().item()\n",
    "        pred_label = predictions[i].cpu().item()\n",
    "        \n",
    "        ax.imshow(img, cmap='gray')\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        ax.set_title(f'T:{true_label} P:{pred_label}', color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Predictions (Green=Correct, Red=Incorrect)', \n",
    "            fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (predictions == test_targets).sum().item()\n",
    "total = len(test_targets)\n",
    "print(f\"Batch Accuracy: {100. * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def calculate_model_size(model, quantized=False):\n",
    "    \"\"\"Calculate model size in bytes\"\"\"\n",
    "    total_bits = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if quantized:\n",
    "            # Ternary: 2 bits per weight\n",
    "            total_bits += param.numel() * 2\n",
    "        else:\n",
    "            # Float32: 32 bits per weight\n",
    "            total_bits += param.numel() * 32\n",
    "    \n",
    "    return total_bits / 8  # Convert to bytes\n",
    "\n",
    "# Calculate sizes\n",
    "float32_size = calculate_model_size(model, quantized=False) / 1024  # KB\n",
    "ternary_size = calculate_model_size(model, quantized=True) / 1024   # KB\n",
    "compression_ratio = float32_size / ternary_size\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "models = ['Float32', 'Ternary']\n",
    "sizes = [float32_size, ternary_size]\n",
    "colors = ['steelblue', 'green']\n",
    "\n",
    "bars = ax1.bar(models, sizes, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Model Size (KB)', fontsize=12)\n",
    "ax1.set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{size:.1f} KB',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Compression visualization\n",
    "ax2.text(0.5, 0.7, f'{compression_ratio:.1f}x', \n",
    "        ha='center', va='center', fontsize=72, fontweight='bold', color='green')\n",
    "ax2.text(0.5, 0.3, 'Compression Ratio', \n",
    "        ha='center', va='center', fontsize=18, color='gray')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SIZE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Float32 Model:  {float32_size:>10.2f} KB\")\n",
    "print(f\"Ternary Model:  {ternary_size:>10.2f} KB\")\n",
    "print(f\"Savings:        {float32_size - ternary_size:>10.2f} KB ({(1 - ternary_size/float32_size)*100:.1f}%)\")\n",
    "print(f\"Compression:    {compression_ratio:>10.1f}x\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps {#next-steps}\n",
    "\n",
    "Congratulations! You've learned the basics of Triton DSL and ternary neural networks.\n",
    "\n",
    "### Continue Learning:\n",
    "\n",
    "1. **ðŸ“˜ [02_quantization_tutorial.ipynb](02_quantization_tutorial.ipynb)**: Deep dive into quantization techniques\n",
    "   - Ternary, INT8, and mixed precision\n",
    "   - QAT vs PTQ comparison\n",
    "   - Advanced quantization strategies\n",
    "\n",
    "2. **ðŸ“˜ [03_performance_analysis.ipynb](03_performance_analysis.ipynb)**: Performance optimization\n",
    "   - Model profiling and benchmarking\n",
    "   - Memory analysis\n",
    "   - Speed optimization techniques\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **Documentation**: [docs/QUICKSTART_PYTORCH_BACKEND.md](../../docs/QUICKSTART_PYTORCH_BACKEND.md)\n",
    "- **Examples**: [examples/](../../examples/)\n",
    "- **API Reference**: [docs/api/](../../docs/api/)\n",
    "\n",
    "### Try This:\n",
    "\n",
    "- Experiment with different network architectures\n",
    "- Try different quantization thresholds\n",
    "- Test on CIFAR-10 or other datasets\n",
    "- Implement stochastic quantization\n",
    "- Compare with binary neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… What Triton DSL is and why ternary neural networks matter  \n",
    "âœ… How to implement ternary quantization with PyTorch  \n",
    "âœ… How to build and train a ternary neural network  \n",
    "âœ… How to visualize quantization and model performance  \n",
    "âœ… The memory savings from ternary quantization (16x!)  \n",
    "\n",
    "**Key Takeaway**: Ternary Neural Networks provide a practical trade-off between model size and accuracy, making them ideal for edge deployment and resource-constrained environments.\n",
    "\n",
    "---\n",
    "\n",
    "*Triton DSL - High-Performance Ternary Neural Networks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
